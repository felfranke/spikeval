# -*- coding: utf-8 -*-
#
# spikeval - module/alignment_old.py
#
# Felix Franke <felfranke@googlemail.com>
# sometime in 2010
#

"""spike train alignment module"""
__docformat__ = 'restructuredtext'
__all__ = ['align_spike_trains', 'similarity', 'simi', 'overlaps',
           'nice_table_from_analysis', 'csv_from_analysis',
           'print_nice_table']


##--- IMPORTS

import scipy as sp
from ..util import dict_arrsort, dict_list2arr


##--- FUNCTIONS

def align_spike_trains(G, E, max_shift=15, max_jitter=6, max_overlap_dist=45):
    """computes the evaluation of spike sorting

    E contains the sorted spike
    trains - given the real/ideal/ground truth spike trains in G

    Calculates the similarity matrix between all pairs of spike trains from
    the
    ground truth and the estimation. This is used to find the optimal
    assignment
    between the spike trains, if one is a ground truth and the other is an
    estimation.

    Assignment Matrix:
        A label is assigned to every estimated spike. The following table
        lists
        all possible labels, given the different configurations is the
        ground truth. We assume E1 was found TO correlate with G1 and E2 is
        corresponding to G2. A "1" indicates a spike w.r.t. shift and jitter.

        =====  === === ===== == == ===== ====== ===== =====
        G1      1        1       1   1                  1
        G2          1    1                 1
        G3                           1     1      1
        =====  === === ===== == == ===== ====== ===== =====
        E1      1   1    1    1            1      1     1
        E2               1                        1     1
        =====  === === ===== == == ===== ====== ===== =====
        label  TP  FPA TPOvp FP FN FNOvp FPAOvp FPOGT

        TP : true positive
            E1 spike is assigned to associated ground truth spike train.
        FPA
            E1 spike is assigned to non associated ground truth spike train.
        TPOvp : true positive and overlap
            E1 spike is assigned to associated ground truth spike train and
            this
            spike participates in an overlap with another ground truth spike
             train.
        FP : false positive
            E1 spike is not assigned to any ground truth spike.
        FN : false negative
            There is no E1 spike for a spike in the associated ground truth
            spike train.
        FNOvp : false negative and overlap
            There is no E1 spike for a spike in the associated ground truth
            spike train and this spike participates in an overlap with another
            ground truth spike train.
        FPAOvp
            E1 spike is assigned to a spike of a non associated
            ground truth spike train and this spike participates in an
            overlap.

    :Returns:
        dict : dict with lots of entries [specify this!]

    :Parameters:
        G : dict of ndarray
            dict containing 1d ndarrays/lists of integers,
            representing the single unit
            spike trains. This is the ground truth.
        E : dict of ndarray
            dict containing 1d ndarrays/lists of integers,
            representing the single unit
            spike trains. this is the estimation.
        maxshift : int
            Upper bound for the tested shift of spike trains towards each
            other
            Default=15
        maxjitter : int
            upper bound for the tested jitter tolerance
            Default=6
        maxoverlapdistance : int
            upper bound for the tested overlap distance
            Default=45
    """
    # DOC: this documentation needs to be more precise on the return part!!

    # inits and checks
    G = dict_list2arr(G)
    E = dict_list2arr(E)
    G = dict_arrsort(G)
    E = dict_arrsort(E)
    n = len(G)
    m = len(E)

    similarity_matrix = sp.zeros((n, m))
    shift_matrix = sp.zeros((n, m))

    rval = {'sfuncs':sp.zeros((n, m, 2 * max_shift + 1))}

    # compute similarity score and optimal shift between all pairs of spike
    # trains
    for i in xrange(n):
        for j in xrange(m):
            sfunc = similarity(G[G.keys()[i]], E[E.keys()[j]], max_shift)
            similarity_matrix[i, j] = sfunc.max()
            shift_matrix[i, j] = sfunc.argmax() - max_shift
            rval['sfuncs'][i, j, :] = sfunc

    # shift all estimated spike trains so that they fit optimal to the best
    # matching true spike train
    u_f2k = sp.zeros(m)
    delta_shift = sp.zeros(m)
    for j in xrange(m):
        myidx = similarity_matrix[:, j].argmax()
        elta_shift[j] = shift_matrix[myidx, j]
        E[E.keys()[j]] = sp.array(E[E.keys()[j]]) + delta_shift[j]

    # sort the spiketrain pairings according to their similarity measure
    # this ensures that the best matching spiketrains will get all
    # the matching spikes. no spike that matches will thus be aligned to
    # another spike train.
    sorted_tupels = []
    S = similarity_matrix.copy()
    for i in xrange(n * m):
        maxidx = S.argmax()
        sorted_tupels.append((int(sp.floor(maxidx / m)), maxidx % m))
        S[sorted_tupels[i]] = -1

    # init alignment dictonary
    alignment = {}
    idx = 0
    for i in xrange(n):
        for j in xrange(m):
            alignment[(G.keys()[i], E.keys()[j])] = []
            idx += 1

    # convert G and E to lists, otherwise we cant remove objects
    GBlocked = {}
    EBlocked = {}
    rval['num_known'] = sp.zeros(n)
    for i in xrange(n):
        rval['num_known'][i] = G[G.keys()[i]].shape[0]
        GBlocked[G.keys()[i]] = sp.zeros(G[G.keys()[i]].shape)
    rval['num_found'] = sp.zeros(m)
    for j in xrange(m):
        rval['num_found'][j] = E[E.keys()[j]].shape[0]
        EBlocked[E.keys()[j]] = sp.zeros(E[E.keys()[j]].shape)

    # GBlocked will contain for every _inserted_ spike a 0 or a 1
    # 0: it was not assigned to any of the found spike trains => false
    # negatives
    # 1: it was assigned to a spike of E => either true positive or false
    #    negative + false positive wrong assignment
    #
    # EBlocked will contain for every _found_ spike a 0 or a 1
    # 0: it was not assigned to any of the inserted spike trains => false
    # positive
    # 1: it was assigned to a spike of G => it will be handled when G is
    # analyzed!

    spike_number_assignment_matrix = sp.zeros((n, m))
    # run over the sorted tupels and block all established spike assignments
    for i in xrange(n * m):
        k1idx = sorted_tupels[i][0]
        k2idx = sorted_tupels[i][1]
        k1 = G.keys()[sorted_tupels[i][0]]
        k2 = E.keys()[sorted_tupels[i][1]]
        train1 = G[k1]
        train2 = E[k2]
        idx1 = 0
        idx2 = 0
        while idx1 < len(train1) and\
              idx2 < len(train2):
            # if a spike is blocked it cannot be associated anymore. jump
            if GBlocked[k1][idx1] == 1:
                idx1 += 1
                continue
            if EBlocked[k2][idx2] == 1:
                idx2 += 1
                continue

            if train1[idx1] <= train2[idx2] + max_jitter and\
               train1[idx1] >= train2[idx2] - max_jitter:
                # spike assignment found, remove spikes
                alignment[(k1, k2)].append((idx1, idx2))
                GBlocked[k1][idx1] = 1
                EBlocked[k2][idx2] = 1

                spike_number_assignment_matrix[k1idx, k2idx] += 1
                # We cannot calculate TP/FP/FNs here, since we dont yet know
                # which G belongs to which E (see next step)
            if train1[idx1] < train2[idx2]:
                idx1 += 1
            else:
                idx2 += 1

    # now establish the one to one relationships between the true and
    # found spike trains. this is a different relationship than the one before
    # because there can maximal be min(n,m) associations. If there are more
    # found spike trains than inserted (m>n), some wont have a partner and
    # be thus treated as FPs. If there are more inserted than found (m>n) than
    # some will be treated as being not found (FNs).

    # Assignment vectors between true and estimated units
    u_k2f = sp.ones(n, dtype=sp.int16) * -1
    u_f2k = sp.ones(m, dtype=sp.int16) * -1

    nAssociations = min(n, m)
    found = 0
    count = 0
    blocked_rows = []
    blocked_cols = []
    snam = spike_number_assignment_matrix.copy()
    while (found < nAssociations) and (count < n * m):
        i, j = U.matrix_argmax(snam)
        if i not in blocked_rows and j not in blocked_cols:
            blocked_rows.append(i)
            u_k2f[i] = j
            blocked_cols.append(j)
            u_f2k[j] = i
            found += 1
        snam[i, j] = -1
        count += 1

    # now we want to calculate FPs and FNs. Since in
    # spike_number_assignment_matrix
    # the assigned spikes are coded and in u_k2f and u_f2k the assignments
    # of the
    # units to each other, we can now compare the number of assignments to
    # the total
    # number of spikes in the corresponding trains. this will directly give
    # the
    # correct/error numbers.

    # mark all the overlapping spikes
    ret = overlaps(G, max_overlap_dist)
    O = ret['O']
    NO = ret['Onums']

    # initialize dictionaries for labels for all spikes
    GL = {}
    for k in G.keys():
        GL[k] = sp.zeros(G[k].shape, dtype=sp.int16)
    EL = {}
    for k in E.keys():
        EL[k] = sp.zeros(E[k].shape, dtype=sp.int16)


    # run over every single spike and check which label it gets
    #             1      2      3     4       5     6      7
    labelList = ['TP', 'TPO', 'FP', 'FPA', 'FPAO', 'FN', 'FNO']
    TP = sp.zeros(n)
    TPO = sp.zeros(n)
    FP = sp.zeros(m) # m !!
    FPA = sp.zeros(n)
    FPAO = sp.zeros(n)
    FPA_E = sp.zeros(m) # m!!
    FPAO_E = sp.zeros(m) # m!!
    FN = sp.zeros(n)
    FNO = sp.zeros(n)
    # handle the spikes which were aligned first
    for i in xrange(n):
        k1 = G.keys()[i]
        for j in xrange(m):
            k2 = E.keys()[j]
            for a in xrange(len(alignment[(k1, k2)])):
                ovp = O[k1][alignment[(k1, k2)][a][0]]
                # labelList = ['TP', 'TPO', 'FP', 'FPA', 'FPAO', 'FN', 'FNO']
                if u_k2f[i] == j:
                    if ovp == 1:
                        GL[k1][alignment[(k1, k2)][a][0]] = 2 # TPO
                        EL[k2][alignment[(k1, k2)][a][1]] = 2
                        TPO[i] += 1
                    else:
                        GL[k1][alignment[(k1, k2)][a][0]] = 1 # TP
                        EL[k2][alignment[(k1, k2)][a][1]] = 1
                        TP[i] += 1
                else:
                    if ovp == 1:
                        GL[k1][alignment[(k1, k2)][a][0]] = 5 #FPAO
                        EL[k2][alignment[(k1, k2)][a][1]] = 5
                        FPAO[i] += 1
                        FPAO_E[
                        j] += 1  # Count assignment errors twice! FP + FN
                    else:
                        GL[k1][alignment[(k1, k2)][a][0]] = 4 # FPA
                        EL[k2][alignment[(k1, k2)][a][1]] = 4
                        FPA[i] += 1
                        FPA_E[j] += 1 # Count assignment errors twice!

        # Now check all spikes of i which have no labels. those are FNs
        for sp in xrange(len(GL[k1])):
            if GL[k1][sp] == 0:
                if O[k1][sp] == 1:
                    GL[k1][sp] = 7  # FNO
                    FNO[i] += 1
                else:
                    GL[k1][sp] = 6  # FN
                    FN[i] += 1
                    # The last thing to do is to check all labels of spikes
                    # in E. Those
                    # which have no label yet are FPs
    for j in xrange(m):
        k2 = E.keys()[j]
        FP[j] = 0
        for sp in xrange(len(EL[k2])):
            if EL[k2][sp] == 0:
                EL[k2][sp] = 3 # FP
                FP[j] += 1


    # Build return _value dictionary
    rval['scores'] = similarity_matrix
    rval['shifts'] = shift_matrix
    rval['delta_shift'] = delta_shift
    rval['alignment'] = alignment
    rval['overlapping'] = O
    rval['spike_number_assignment_matrix'] = spike_number_assignment_matrix
    rval['EL'] = EL
    rval['GL'] = GL

    rval['TP'] = TP
    rval['TPO'] = TPO
    rval['FPA'] = FPA
    rval['FPAO'] = FPAO
    rval['FN'] = FN
    rval['FNO'] = FNO
    rval['FP'] = FP

    rval['u_k2f'] = u_k2f
    rval['u_f2k'] = u_f2k

    # build pretty table
    rval['table'] = []
    rval['table'].append(['GT Unit ID', # ID of known Unit
                          'Found Unit ID', # ID of associated found Unit
                          'Known Spikes', # Number of Spikes of Known Unit
                          'Overlapping Spikes',
                          # Number of those Spikes participating in an Overlap
                          'Found Spikes', # Number of Spikes of Found Unit
                          'True Pos',
                          # Number of those Spikes which are assigned to
                          # Spikes
                          # of the associated known Unit.
                          'True Pos Ovps', #
                          'False Pos Assign GT',
                          # Number of Spikes of Found Unit which are
                          # assigned to a non-associated Unit
                          'False Pos Assign Found',
                          'False Pos Ovps GT', # FPAO
                          'FPs Assign Ovps Found', # FPAO_E
                          'False Neg',
                          'False Neg Overlaps',
                          'False Pos'])

    # Build Table with one row for every assignment of two spike trains and
    # one row for every unassigned spike train.
    # TODO: The number of false positive assignments for one of the assignment
    # rows has to be the sum of the individual assignement errors.
    # ???? ->This way an assignment error counts as 2 errors (one FP and one
    # FN).

    remaining_found_units = sp.ones(m)
    # Build the assignment rows and unassigned ground truth spike train rows
    # first
    for i in xrange(n):
        unitk = G.keys()[i]
        known = rval['num_known'][i]
        overlapping = NO[i]
        tp = TP[i]
        tpo = TPO[i]
        fn = FN[i]
        fno = FNO[i]
        fpa = FPA[i]
        fpao = FPAO[i]

        j = rval['u_k2f'][i]
        unitf = ''
        found = fp = fpae = fpaoe = 0
        if j >= 0:
            remaining_found_units[j] = 0
            unitf = E.keys()[j]
            found = rval['num_found'][j]
            fp = FP[j]
            fpae = FPA_E[j]
            fpaoe = FPAO_E[j]

        rval['table'].append([unitk, unitf, known, overlapping, found, tp,
                              tpo, fpa, fpae, fpao, fpaoe, fn, fno, fp])

    # Append "False Positive Unit" which has all found spikes of found units
    # which
    # were not assigned to a ground truth unit
    for j in xrange(m):
        if remaining_found_units[j] == 1:
            unitk = ''
            known = overlapping = tp = tpo = fn = fno = fpa = fpao = 0
            unitf = E.keys()[j]
            found = rval['num_found'][j]
            fp = FP[j]
            fpae = FPA_E[j]
            fpaoe = FPAO_E[j]
            rval['table'].append([unitk, unitf, known, overlapping, found, tp,
                                  tpo, fpa, fpae, fpao, fpaoe, fn, fno, fp])

    return rval


def similarity(st1, st2, mtau):
    """
    Calculates the cross correlation function between two spike trains
    """
    sfunc = sp.zeros(2 * mtau + 1)
    for tau in xrange(-mtau, mtau + 1):
        sfunc[tau + mtau] = simi(st1, st2 + tau)
    return sfunc


def simi(s1, s2):
    """
    Calculates the normalized scalar product between two binary vectors
    which are given by two point processes without actually creating the
    binary vectors.
    """
    p = 0
    s1idx = 0
    s2idx = 0
    while s1idx < s1.shape[0] and s2idx < s2.shape[0]:
        if s1[s1idx] == s2[s2idx]:
            p += 1

        if s1[s1idx] < s2[s2idx]:
            s1idx += 1
        else:
            s2idx += 1

    return 2.0 * p / (s1.shape[0] + s2.shape[0])


def overlaps(G, window):
    """
    Calculates a "boolean" dictonary, indicating for every spike in every
    spiketrain in G whether it belongs to an overlap or not
    """
    n = len(G)
    O = {}
    for k in G.keys():
        O[k] = sp.zeros(G[k].shape, dtype=sp.bool_)
    Onums = sp.zeros(len(G))
    # run over all pairs of spike trains in G
    for i in xrange(n):
        for j in xrange(i + 1, n):
            # for every pair run over all spikes in i and check whether a
            # spike
            # in j overlaps
            trainI = G[G.keys()[i]]
            trainJ = G[G.keys()[j]]
            idxI = 0
            idxJ = 0
            while idxI < len(trainI) and idxJ < len(trainJ):
                # Overlapping?
                if abs(trainI[idxI] - trainJ[idxJ]) < window:
                    # Every spike can only be in one or no overlap. prevents
                    # triple
                    # counting
                    if O[G.keys()[i]][idxI] == 0:
                        O[G.keys()[i]][idxI] = 1
                        Onums[i] += 1
                    if O[G.keys()[j]][idxJ] == 0:
                        O[G.keys()[j]][idxJ] = 1
                        Onums[j] += 1

                if trainI[idxI] < trainJ[idxJ]:
                    idxI += 1
                else:
                    idxJ += 1
    ret = {'O':O, 'Onums':Onums}
    return ret

# -------------------------------------------------------------------
# Merges 2 dictonaries of spike trains and updates the corresponding alignment
#def merge_spiketrains(G, E, alignment = None):
#    rval = {}
#    for i in xrange(G.keys.shape[0]):
#        rval[G.keys[i]+100] = G[G.keys[i]]
#    for i in xrange(E.keys.shape[0]):
#        rval[E.keys[i]+200] = E[E.keys[i]]
#
#    if alignment is not None:

HSTRS = ['GT ID', 'FU ID', 'KS', 'OS', 'FS', 'TP', 'TPO', 'FPAE', 'FPAO',
         'FPAOE', 'FN', 'FNO', 'FP']

def nice_table_from_analysis(ana, space=6):
    """yields a nicely readable string that contains the information about
    the performance of that sorting"""

    rval = HSTRS[0].center(space) + ' - ' + HSTRS[1].center(space) + ' | '
    form = '%{}s - %{}s | '.format(space, space)
    for item in HSTRS[2:]:
        rval += item.center(6) + ' '
        form += '%{}d '.format(space)
    rval += '\n'
    form += '\n'
    for i in xrange(1, len(ana['table'])):
        rval += form % tuple(ana['table'][i][0:14])
    return rval


def csv_from_analysis(ana, header=True, space=6):
    """yields a string that can be stored as a .csv file"""

    rval = ''
    if header:
        rval += ''.join(map(str.center, HSTRS, len(HSTRS) * [space])) + '\n'
    form = ', '.join(['%{}s'.format(space)] * 2 + ['%{}d'] * (len(HSTRS)) - 2)
    form += '\n'
    for i in xrange(1, len(ana['table'])):
        rval += form % tuple(ana['table'][i][0:14])
    return rval


def print_nice_table(ret):
    print nice_table_from_analysis(ret)

##--- MAIN

if __name__ == '__main__':
    G = {'0':sp.array([1, 100, 200, 301, 400]),
         1:sp.array([50, 150, 250, 303, 350])}

    E = {0:sp.array([1, 100, 200, 305, 307]),
         1:sp.array([50, 150, 250, 590, 550, 648, 720])}

    ret = align_spike_trains(G, E, max_shift=2, max_jitter=12)
    from plot import P, spike_trains

    fig = P.figure(facecolor='white')
    spike_trains(G, spiketrains2=E, alignment=ret['alignment'],
                 label1=ret['GL'], label2=ret['EL'], plot_handle=fig,
                 samples_per_second=16000)
    print 'Done Plot 0.'
    print ret['alignment']

#    G = {}
#    G['0'] = sp.array([40, 80, 90, 170, 400])
#    G[1] = sp.array([42, 150, 180, 190, 350])
#    G[2] = sp.array([39, 80, 150, 405])
#
#    E = {}
#    E[0] = sp.array([42, 80, 170, 401])
#    E[1] = sp.array([40, 90, 150, 180, 190, 250, 348, 420])
#
#    #ret = align_spike_trains(G, E, maxshift=2, maxjitter=2)
#    # print ret
#    import common.plot as plot
#
#    #print 'lala 1'
#    #plot.spike_trains(G, spiketrains2=E, alignment=ret['alignment'], show=0)
#
#    print 'Done Plot 1.'
#
#    G = {}
#    G['0'] = sp.array([40, 80, 90, 170, 400])
#    G[1] = sp.array([42, 150, 180, 190, 350])
#    G[2] = sp.array([39, 80, 150, 405])
#
#    E = {}
#    E[3] = sp.array([42, 80, 170, 401])
#    E['Multi Unit 1'] = sp.array([40, 90, 150, 180, 190, 250, 348, 420])
#    E[4] = sp.array([37, 79, 96, 149, 201, 405])
#    E['Multi Unit 2'] = sp.array([10, 20, 30, 40, 50, 60, 60, 170])
#
#    ret = align_spike_trains(G, E, maxshift=2, maxjitter=2,
#                             maxoverlapdistance=5)
#    print ret
#
#    from plot import plt, spiketrains
#
#    fig = plt.figure(facecolor='white')
#    spike_trains(G, spiketrains2=E, alignment=ret['alignment'],
#                 label1=ret['GL'], label2=ret['EL'], plot_handle=fig,
#                 samples_per_second=16000)
#    print 'Done Plot 2.'
#
